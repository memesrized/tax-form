{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "477be8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tax_form.eval.data import load_predicted_jsonl, load_ground_truth_json\n",
    "from tax_form.eval.metrics import aggregate_metrics_across_documents\n",
    "\n",
    "pred_path = Path(\"../data/output/\")\n",
    "true_path = Path(\"../data/target/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33d55286",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = {x.stem: load_predicted_jsonl(x) for x in pred_path.glob(\"*.jsonl\")}\n",
    "gt = {x.stem: load_ground_truth_json(x) for x in true_path.glob(\"*.json\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a305b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MICRO-AVERAGED METRICS (Overall - Exact Match)\n",
      "============================================================\n",
      "Precision: 0.8475\n",
      "Recall:    0.7937\n",
      "F1 Score:  0.8197\n",
      "\n",
      "True Positives:  50\n",
      "False Positives: 9\n",
      "False Negatives: 13\n",
      "\n",
      "\n",
      "============================================================\n",
      "MACRO-AVERAGED METRICS (Mean per Document - Exact Match)\n",
      "============================================================\n",
      "Precision: 0.7726\n",
      "Recall:    0.7464\n",
      "F1 Score:  0.7571\n",
      "\n",
      "Number of Documents: 10\n"
     ]
    }
   ],
   "source": [
    "# Compute overall metrics across all documents\n",
    "\n",
    "# Micro-averaging: overall metrics (sum all TP/FP/FN)\n",
    "micro_metrics = aggregate_metrics_across_documents(\n",
    "    pred, gt, match_type=\"exact\", aggregation=\"micro\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MICRO-AVERAGED METRICS (Overall - Exact Match)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {micro_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {micro_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {micro_metrics['f1']:.4f}\")\n",
    "print(f\"\\nTrue Positives:  {micro_metrics['true_positives']}\")\n",
    "print(f\"False Positives: {micro_metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {micro_metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Macro-averaging: mean of per-document scores\n",
    "macro_metrics = aggregate_metrics_across_documents(\n",
    "    pred, gt, match_type=\"exact\", aggregation=\"macro\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MACRO-AVERAGED METRICS (Mean per Document - Exact Match)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {macro_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {macro_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {macro_metrics['f1']:.4f}\")\n",
    "print(f\"\\nNumber of Documents: {macro_metrics['num_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9500188",
   "metadata": {},
   "source": [
    "Metrics are the same, but there are just no overlaps I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad4ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MICRO-AVERAGED METRICS (Overall - Overlap Match)\n",
      "============================================================\n",
      "Precision: 0.8475\n",
      "Recall:    0.7937\n",
      "F1 Score:  0.8197\n",
      "\n",
      "True Positives:  50\n",
      "False Positives: 9\n",
      "False Negatives: 13\n",
      "\n",
      "\n",
      "============================================================\n",
      "MACRO-AVERAGED METRICS (Mean per Document - Overlap Match)\n",
      "============================================================\n",
      "Precision: 0.7726\n",
      "Recall:    0.7464\n",
      "F1 Score:  0.7571\n",
      "\n",
      "Number of Documents: 10\n"
     ]
    }
   ],
   "source": [
    "# Compute overall metrics across all documents\n",
    "\n",
    "# Micro-averaging: overall metrics (sum all TP/FP/FN)\n",
    "micro_metrics = aggregate_metrics_across_documents(\n",
    "    pred, gt, match_type=\"overlap\", aggregation=\"micro\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MICRO-AVERAGED METRICS (Overall - Overlap Match)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {micro_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {micro_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {micro_metrics['f1']:.4f}\")\n",
    "print(f\"\\nTrue Positives:  {micro_metrics['true_positives']}\")\n",
    "print(f\"False Positives: {micro_metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {micro_metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Macro-averaging: mean of per-document scores\n",
    "macro_metrics = aggregate_metrics_across_documents(\n",
    "    pred, gt, match_type=\"overlap\", aggregation=\"macro\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MACRO-AVERAGED METRICS (Mean per Document - Overlap Match)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {macro_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {macro_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {macro_metrics['f1']:.4f}\")\n",
    "print(f\"\\nNumber of Documents: {macro_metrics['num_documents']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
